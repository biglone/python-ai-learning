{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第3课：RNN 循环神经网络\n",
    "\n",
    "## 学习目标\n",
    "- 理解序列数据和 RNN 原理\n",
    "- 掌握 LSTM 和 GRU 结构\n",
    "- 学会处理时序数据\n",
    "- 完成序列预测任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 循环神经网络简介\n",
    "\n",
    "RNN 专门处理序列数据，通过循环连接传递历史信息。\n",
    "\n",
    "**应用场景**：\n",
    "- 自然语言处理\n",
    "- 时间序列预测\n",
    "- 语音识别\n",
    "- 机器翻译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 基本 RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN 单元原理\n",
    "class SimpleRNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(SimpleRNNCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # 输入到隐藏状态\n",
    "        self.i2h = nn.Linear(input_size, hidden_size)\n",
    "        # 隐藏状态到隐藏状态\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        # h_t = tanh(W_ih * x_t + W_hh * h_{t-1})\n",
    "        hidden = torch.tanh(self.i2h(x) + self.h2h(hidden))\n",
    "        return hidden\n",
    "\n",
    "# 测试\n",
    "rnn_cell = SimpleRNNCell(input_size=10, hidden_size=20)\n",
    "x = torch.randn(1, 10)  # (batch, input_size)\n",
    "h = torch.zeros(1, 20)  # (batch, hidden_size)\n",
    "\n",
    "for t in range(5):\n",
    "    h = rnn_cell(x, h)\n",
    "    print(f\"时间步 {t}: 隐藏状态形状 = {h.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 PyTorch 内置 RNN\n",
    "rnn = nn.RNN(input_size=10, hidden_size=20, num_layers=2, batch_first=True)\n",
    "\n",
    "# 输入: (batch, seq_len, input_size)\n",
    "x = torch.randn(3, 5, 10)  # 3个样本，序列长度5，特征维度10\n",
    "\n",
    "# 输出和最终隐藏状态\n",
    "output, h_n = rnn(x)\n",
    "\n",
    "print(f\"输入形状: {x.shape}\")\n",
    "print(f\"输出形状: {output.shape}\")\n",
    "print(f\"最终隐藏状态形状: {h_n.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LSTM（长短期记忆网络）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 原理图解\n",
    "print(\"\"\"\n",
    "LSTM 单元结构:\n",
    "\n",
    "      ┌─────────────────────────────────────────┐\n",
    "      │                  Cell State             │\n",
    "      │    ──────────────────────────────►      │\n",
    "      │          ×         +                    │\n",
    "      │          │         │                    │\n",
    "      │    ┌─────┴─────┐   │                    │\n",
    "      │    │  Forget   │   │                    │\n",
    "      │    │   Gate    │   │                    │\n",
    "      │    └───────────┘   │                    │\n",
    "      │          σ         │                    │\n",
    "      │          │   ┌─────┴─────┐              │\n",
    "      │          │   │   Input   │×  tanh      │\n",
    "      │          │   │   Gate    │              │\n",
    "      │          │   └───────────┘              │\n",
    "      │          │         σ                    │\n",
    "      │    ┌─────┴─────────┴─────┐              │\n",
    " h_{t-1}──►│                     ├──────► h_t   │\n",
    "      │    │      Concat         │              │\n",
    "   x_t ───►│                     │              │\n",
    "      │    └─────────────────────┘              │\n",
    "      │                   │                     │\n",
    "      │             ┌─────┴─────┐               │\n",
    "      │             │  Output   │×  tanh       │\n",
    "      │             │   Gate    │               │\n",
    "      │             └───────────┘               │\n",
    "      │                   σ                     │\n",
    "      └─────────────────────────────────────────┘\n",
    "\n",
    "三个门:\n",
    "- 遗忘门 (Forget Gate): 决定丢弃哪些信息\n",
    "- 输入门 (Input Gate): 决定存储哪些新信息\n",
    "- 输出门 (Output Gate): 决定输出哪些信息\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 PyTorch LSTM\n",
    "lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=2, batch_first=True)\n",
    "\n",
    "x = torch.randn(3, 5, 10)  # (batch, seq_len, input_size)\n",
    "\n",
    "# 输出和最终状态 (h_n: 隐藏状态, c_n: 细胞状态)\n",
    "output, (h_n, c_n) = lstm(x)\n",
    "\n",
    "print(f\"输入形状: {x.shape}\")\n",
    "print(f\"输出形状: {output.shape}\")\n",
    "print(f\"最终隐藏状态形状: {h_n.shape}\")\n",
    "print(f\"最终细胞状态形状: {c_n.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GRU（门控循环单元）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "GRU 相比 LSTM:\n",
    "- 只有两个门：重置门和更新门\n",
    "- 没有单独的细胞状态\n",
    "- 参数更少，计算更快\n",
    "- 在某些任务上效果相当\n",
    "\"\"\")\n",
    "\n",
    "# 使用 PyTorch GRU\n",
    "gru = nn.GRU(input_size=10, hidden_size=20, num_layers=2, batch_first=True)\n",
    "\n",
    "x = torch.randn(3, 5, 10)\n",
    "output, h_n = gru(x)\n",
    "\n",
    "print(f\"GRU 输出形状: {output.shape}\")\n",
    "print(f\"GRU 最终隐藏状态形状: {h_n.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 时间序列预测示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成正弦波数据\n",
    "def generate_sin_data(seq_length, num_samples):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        start = np.random.uniform(0, 2 * np.pi)\n",
    "        x = np.sin(np.linspace(start, start + 4 * np.pi, seq_length + 1))\n",
    "        X.append(x[:-1])\n",
    "        y.append(x[1:])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# 生成数据\n",
    "seq_length = 50\n",
    "X_train, y_train = generate_sin_data(seq_length, 1000)\n",
    "X_test, y_test = generate_sin_data(seq_length, 100)\n",
    "\n",
    "# 转换为张量\n",
    "X_train = torch.FloatTensor(X_train).unsqueeze(-1)  # (batch, seq_len, 1)\n",
    "y_train = torch.FloatTensor(y_train).unsqueeze(-1)\n",
    "X_test = torch.FloatTensor(X_test).unsqueeze(-1)\n",
    "y_test = torch.FloatTensor(y_test).unsqueeze(-1)\n",
    "\n",
    "print(f\"训练数据形状: {X_train.shape}\")\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(X_train[0].squeeze(), label='输入')\n",
    "plt.plot(y_train[0].squeeze(), label='目标')\n",
    "plt.legend()\n",
    "plt.title('时间序列预测任务')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 时间序列预测模型\n",
    "class LSTMPredictor(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, output_size=1):\n",
    "        super(LSTMPredictor, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, \n",
    "                           batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, input_size)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # lstm_out: (batch, seq_len, hidden_size)\n",
    "        output = self.fc(lstm_out)\n",
    "        # output: (batch, seq_len, output_size)\n",
    "        return output\n",
    "\n",
    "model = LSTMPredictor().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 创建 DataLoader\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 训练\n",
    "num_epochs = 20\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_x)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.6f}')\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('训练损失')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估模型\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_device = X_test.to(device)\n",
    "    predictions = model(X_test_device).cpu()\n",
    "\n",
    "# 可视化预测结果\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.plot(y_test[i].squeeze(), label='真实值', linewidth=2)\n",
    "    ax.plot(predictions[i].squeeze(), '--', label='预测值', linewidth=2)\n",
    "    ax.legend()\n",
    "    ax.set_title(f'样本 {i+1}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 计算测试损失\n",
    "test_loss = criterion(predictions, y_test)\n",
    "print(f\"测试 MSE: {test_loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 文本分类示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单的文本分类模型\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=2, \n",
    "                           batch_first=True, bidirectional=True, dropout=0.3)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # *2 因为双向\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len)\n",
    "        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(embedded)\n",
    "        \n",
    "        # 使用最后一个时间步的隐藏状态\n",
    "        # h_n: (num_layers*2, batch, hidden_dim) for bidirectional\n",
    "        # 连接前向和后向的最后一层\n",
    "        hidden = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        \n",
    "        output = self.dropout(hidden)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "# 模型示例\n",
    "text_model = TextClassifier(vocab_size=10000, embed_dim=128, \n",
    "                            hidden_dim=256, num_classes=2)\n",
    "print(text_model)\n",
    "print(f\"\\n参数量: {sum(p.numel() for p in text_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模拟文本分类\n",
    "# 假设已经将文本转换为索引序列\n",
    "batch_size = 4\n",
    "seq_len = 20\n",
    "vocab_size = 10000\n",
    "\n",
    "# 模拟输入\n",
    "fake_text = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "fake_labels = torch.randint(0, 2, (batch_size,))\n",
    "\n",
    "# 前向传播\n",
    "output = text_model(fake_text)\n",
    "print(f\"输入形状: {fake_text.shape}\")\n",
    "print(f\"输出形状: {output.shape}\")\n",
    "print(f\"预测: {torch.softmax(output, dim=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 双向 RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 双向 LSTM\n",
    "bi_lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=2, \n",
    "                  batch_first=True, bidirectional=True)\n",
    "\n",
    "x = torch.randn(3, 5, 10)\n",
    "output, (h_n, c_n) = bi_lstm(x)\n",
    "\n",
    "print(\"双向 LSTM:\")\n",
    "print(f\"  输入形状: {x.shape}\")\n",
    "print(f\"  输出形状: {output.shape}  # hidden_size * 2\")\n",
    "print(f\"  h_n 形状: {h_n.shape}  # num_layers * 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 注意力机制简介"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单的注意力机制\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, lstm_output):\n",
    "        # lstm_output: (batch, seq_len, hidden_dim)\n",
    "        \n",
    "        # 计算注意力分数\n",
    "        attention_scores = self.attention(lstm_output)  # (batch, seq_len, 1)\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1)\n",
    "        \n",
    "        # 加权求和\n",
    "        context = torch.sum(attention_weights * lstm_output, dim=1)\n",
    "        \n",
    "        return context, attention_weights\n",
    "\n",
    "# 带注意力的 LSTM 分类器\n",
    "class AttentionLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super(AttentionLSTM, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        context, attention_weights = self.attention(lstm_out)\n",
    "        output = self.fc(context)\n",
    "        return output, attention_weights\n",
    "\n",
    "# 测试\n",
    "attn_model = AttentionLSTM(vocab_size=1000, embed_dim=64, hidden_dim=128, num_classes=2)\n",
    "x = torch.randint(0, 1000, (2, 10))\n",
    "output, attn_weights = attn_model(x)\n",
    "\n",
    "print(f\"输出形状: {output.shape}\")\n",
    "print(f\"注意力权重形状: {attn_weights.shape}\")\n",
    "print(f\"注意力权重 (样本1): {attn_weights[0].squeeze()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 练习题\n",
    "\n",
    "### 练习：使用 LSTM 预测股票价格（模拟数据）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成模拟股票数据\n",
    "np.random.seed(42)\n",
    "n_days = 500\n",
    "stock_prices = 100 + np.cumsum(np.random.randn(n_days) * 2)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(stock_prices)\n",
    "plt.title('模拟股票价格')\n",
    "plt.xlabel('天数')\n",
    "plt.ylabel('价格')\n",
    "plt.show()\n",
    "\n",
    "# 在这里编写代码\n",
    "# 1. 准备训练数据（使用过去 N 天预测下一天）\n",
    "# 2. 构建 LSTM 模型\n",
    "# 3. 训练模型\n",
    "# 4. 可视化预测结果\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 本课小结\n",
    "\n",
    "1. **RNN**：处理序列数据，存在梯度消失问题\n",
    "2. **LSTM**：通过门控机制解决长期依赖\n",
    "3. **GRU**：简化版 LSTM，参数更少\n",
    "4. **双向 RNN**：同时考虑前后文\n",
    "5. **注意力机制**：动态关注重要位置\n",
    "6. **应用**：时序预测、文本分类、序列标注"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
