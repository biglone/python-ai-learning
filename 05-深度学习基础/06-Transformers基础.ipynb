{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第6课：Transformers 基础\n",
    "\n",
    "## 学习目标\n",
    "- 理解 Attention 机制原理\n",
    "- 掌握 Transformer 架构\n",
    "- 了解 BERT、GPT 等预训练模型\n",
    "- 学会使用 Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 注意力机制（Attention）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "注意力机制的核心思想:\n",
    "\n",
    "Attention(Q, K, V) = softmax(QK^T / √d_k) V\n",
    "\n",
    "- Q (Query): 查询向量，表示"我在找什么"\n",
    "- K (Key): 键向量，表示"我有什么"\n",
    "- V (Value): 值向量，表示"实际的内容"\n",
    "- d_k: 键的维度，用于缩放\n",
    "\n",
    "计算步骤:\n",
    "1. 计算 Q 和 K 的点积\n",
    "2. 除以 √d_k 进行缩放\n",
    "3. 应用 softmax 得到注意力权重\n",
    "4. 用权重对 V 加权求和\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 缩放点积注意力\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Q, K, V: (batch, seq_len, d_k)\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "    \n",
    "    # 计算注意力分数\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    \n",
    "    # 应用掩码（可选）\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    # Softmax\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # 加权求和\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# 测试\n",
    "batch_size, seq_len, d_k = 2, 4, 8\n",
    "Q = torch.randn(batch_size, seq_len, d_k)\n",
    "K = torch.randn(batch_size, seq_len, d_k)\n",
    "V = torch.randn(batch_size, seq_len, d_k)\n",
    "\n",
    "output, attention = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"Q/K/V 形状: {Q.shape}\")\n",
    "print(f\"输出形状: {output.shape}\")\n",
    "print(f\"注意力权重形状: {attention.shape}\")\n",
    "print(f\"\\n注意力权重 (样本1):\\n{attention[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化注意力权重\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(attention[0].detach().numpy(), cmap='Blues')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Key 位置')\n",
    "plt.ylabel('Query 位置')\n",
    "plt.title('注意力权重热力图')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 多头注意力（Multi-Head Attention）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # 线性变换层\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        batch_size = Q.size(0)\n",
    "        \n",
    "        # 线性变换\n",
    "        Q = self.W_q(Q)\n",
    "        K = self.W_k(K)\n",
    "        V = self.W_v(V)\n",
    "        \n",
    "        # 分割成多头\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        # 形状: (batch, num_heads, seq_len, d_k)\n",
    "        \n",
    "        # 计算注意力\n",
    "        attn_output, attention = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # 合并多头\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # 输出线性变换\n",
    "        output = self.W_o(attn_output)\n",
    "        \n",
    "        return output, attention\n",
    "\n",
    "# 测试多头注意力\n",
    "mha = MultiHeadAttention(d_model=64, num_heads=8)\n",
    "x = torch.randn(2, 10, 64)  # (batch, seq_len, d_model)\n",
    "\n",
    "output, attention = mha(x, x, x)\n",
    "print(f\"输入形状: {x.shape}\")\n",
    "print(f\"输出形状: {output.shape}\")\n",
    "print(f\"注意力形状: {attention.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 位置编码（Positional Encoding）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "# 可视化位置编码\n",
    "pe = PositionalEncoding(d_model=64, max_len=100)\n",
    "pos_encoding = pe.pe.squeeze().numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(pos_encoding[:50, :], cmap='RdBu', aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.xlabel('编码维度')\n",
    "plt.ylabel('位置')\n",
    "plt.title('位置编码可视化')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transformer 编码器层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # 自注意力 + 残差连接 + 层归一化\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # 前馈网络 + 残差连接 + 层归一化\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# 测试编码器层\n",
    "encoder_layer = TransformerEncoderLayer(d_model=64, num_heads=8, d_ff=256)\n",
    "x = torch.randn(2, 10, 64)\n",
    "output = encoder_layer(x)\n",
    "print(f\"输入形状: {x.shape}\")\n",
    "print(f\"输出形状: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整的 Transformer 编码器\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, max_len=512, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # 嵌入 + 位置编码\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 堆叠的编码器层\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# 测试完整编码器\n",
    "encoder = TransformerEncoder(\n",
    "    vocab_size=10000,\n",
    "    d_model=64,\n",
    "    num_heads=8,\n",
    "    d_ff=256,\n",
    "    num_layers=4\n",
    ")\n",
    "\n",
    "x = torch.randint(0, 10000, (2, 20))  # (batch, seq_len)\n",
    "output = encoder(x)\n",
    "\n",
    "print(f\"输入形状: {x.shape}\")\n",
    "print(f\"输出形状: {output.shape}\")\n",
    "print(f\"参数量: {sum(p.numel() for p in encoder.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transformer 分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, num_classes, max_len=512):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        \n",
    "        self.encoder = TransformerEncoder(\n",
    "            vocab_size, d_model, num_heads, d_ff, num_layers, max_len\n",
    "        )\n",
    "        \n",
    "        # 分类头\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 编码\n",
    "        encoded = self.encoder(x)  # (batch, seq_len, d_model)\n",
    "        \n",
    "        # 使用 [CLS] 位置（第一个位置）或平均池化\n",
    "        pooled = encoded.mean(dim=1)  # (batch, d_model)\n",
    "        \n",
    "        # 分类\n",
    "        output = self.classifier(pooled)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# 创建分类器\n",
    "classifier = TransformerClassifier(\n",
    "    vocab_size=10000,\n",
    "    d_model=128,\n",
    "    num_heads=8,\n",
    "    d_ff=512,\n",
    "    num_layers=4,\n",
    "    num_classes=2\n",
    ")\n",
    "\n",
    "x = torch.randint(0, 10000, (4, 32))\n",
    "output = classifier(x)\n",
    "\n",
    "print(f\"输入形状: {x.shape}\")\n",
    "print(f\"输出形状: {output.shape}\")\n",
    "print(f\"参数量: {sum(p.numel() for p in classifier.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 使用 Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装: pip install transformers\n",
    "\n",
    "# 使用示例代码（需要安装 transformers 库）\n",
    "huggingface_example = '''\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# 1. 使用 pipeline（最简单）\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"I love this product!\")\n",
    "print(result)  # [{'label': 'POSITIVE', 'score': 0.999}]\n",
    "\n",
    "# 2. 手动加载模型和分词器\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 编码文本\n",
    "inputs = tokenizer(\"Hello, world!\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# 3. 微调预训练模型\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    learning_rate=2e-5\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "'''\n",
    "\n",
    "print(\"Hugging Face Transformers 使用示例:\")\n",
    "print(huggingface_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 预训练模型概览"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "常见的预训练 Transformer 模型:\n",
    "\n",
    "1. BERT (Bidirectional Encoder Representations from Transformers)\n",
    "   - 编码器架构\n",
    "   - 双向上下文理解\n",
    "   - 适合分类、NER、问答等任务\n",
    "   - 变体: RoBERTa, ALBERT, DistilBERT\n",
    "\n",
    "2. GPT (Generative Pre-trained Transformer)\n",
    "   - 解码器架构\n",
    "   - 自回归生成\n",
    "   - 适合文本生成任务\n",
    "   - GPT-2, GPT-3, GPT-4\n",
    "\n",
    "3. T5 (Text-to-Text Transfer Transformer)\n",
    "   - 编码器-解码器架构\n",
    "   - 将所有任务转化为文本到文本\n",
    "   - 适合翻译、摘要等任务\n",
    "\n",
    "4. BART\n",
    "   - 编码器-解码器架构\n",
    "   - 结合 BERT 和 GPT 的优点\n",
    "   - 适合生成和理解任务\n",
    "\n",
    "5. 中文模型\n",
    "   - BERT-Chinese\n",
    "   - RoBERTa-wwm-ext\n",
    "   - ERNIE\n",
    "   - MacBERT\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. PyTorch 内置 Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 PyTorch 内置的 Transformer\n",
    "transformer_model = nn.Transformer(\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    dim_feedforward=2048,\n",
    "    dropout=0.1,\n",
    "    batch_first=True\n",
    ")\n",
    "\n",
    "# 输入\n",
    "src = torch.randn(2, 10, 512)  # (batch, src_seq_len, d_model)\n",
    "tgt = torch.randn(2, 20, 512)  # (batch, tgt_seq_len, d_model)\n",
    "\n",
    "# 前向传播\n",
    "output = transformer_model(src, tgt)\n",
    "\n",
    "print(f\"源序列形状: {src.shape}\")\n",
    "print(f\"目标序列形状: {tgt.shape}\")\n",
    "print(f\"输出形状: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用内置编码器\n",
    "encoder_layer = nn.TransformerEncoderLayer(\n",
    "    d_model=256,\n",
    "    nhead=8,\n",
    "    dim_feedforward=1024,\n",
    "    dropout=0.1,\n",
    "    batch_first=True\n",
    ")\n",
    "\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "\n",
    "src = torch.randn(4, 32, 256)\n",
    "output = transformer_encoder(src)\n",
    "\n",
    "print(f\"编码器输出形状: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 练习题\n",
    "\n",
    "### 练习：使用 Transformer 进行文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在这里编写代码\n",
    "# 1. 创建一个简单的文本分类数据集\n",
    "# 2. 使用我们实现的 TransformerClassifier\n",
    "# 3. 训练模型并评估性能\n",
    "# 4. 可选: 尝试使用 Hugging Face 的预训练模型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 本课小结\n",
    "\n",
    "1. **注意力机制**：Query-Key-Value，动态加权\n",
    "2. **多头注意力**：并行多个注意力头\n",
    "3. **位置编码**：注入位置信息\n",
    "4. **Transformer 架构**：注意力 + FFN + 残差 + LayerNorm\n",
    "5. **预训练模型**：BERT、GPT、T5 等\n",
    "6. **Hugging Face**：便捷的预训练模型库"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
