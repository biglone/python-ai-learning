{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第5课：NLP 自然语言处理入门\n",
    "\n",
    "## 学习目标\n",
    "- 理解文本预处理流程\n",
    "- 掌握词嵌入技术\n",
    "- 学会构建文本分类模型\n",
    "- 了解常见 NLP 任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 文本预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例文本\n",
    "texts = [\n",
    "    \"I love this movie! It's amazing.\",\n",
    "    \"This film is terrible, I hated it.\",\n",
    "    \"Great acting and wonderful story.\",\n",
    "    \"Boring and waste of time.\",\n",
    "    \"Best movie I've ever seen!\",\n",
    "    \"Don't watch this, it's awful.\"\n",
    "]\n",
    "labels = [1, 0, 1, 0, 1, 0]  # 1=正面, 0=负面\n",
    "\n",
    "# 文本清洗函数\n",
    "def clean_text(text):\n",
    "    # 转小写\n",
    "    text = text.lower()\n",
    "    # 移除标点符号\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # 分词\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# 清洗所有文本\n",
    "cleaned_texts = [clean_text(text) for text in texts]\n",
    "print(\"清洗后的文本:\")\n",
    "for i, (original, cleaned) in enumerate(zip(texts, cleaned_texts)):\n",
    "    print(f\"  原始: {original}\")\n",
    "    print(f\"  清洗: {cleaned}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建词汇表\n",
    "class Vocabulary:\n",
    "    def __init__(self, min_freq=1):\n",
    "        self.word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.idx2word = {0: '<PAD>', 1: '<UNK>'}\n",
    "        self.word_freq = Counter()\n",
    "        self.min_freq = min_freq\n",
    "    \n",
    "    def build_vocab(self, texts):\n",
    "        # 统计词频\n",
    "        for tokens in texts:\n",
    "            self.word_freq.update(tokens)\n",
    "        \n",
    "        # 添加词到词汇表\n",
    "        for word, freq in self.word_freq.items():\n",
    "            if freq >= self.min_freq and word not in self.word2idx:\n",
    "                idx = len(self.word2idx)\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word[idx] = word\n",
    "    \n",
    "    def encode(self, tokens):\n",
    "        return [self.word2idx.get(token, 1) for token in tokens]  # 1 是 <UNK>\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        return [self.idx2word.get(idx, '<UNK>') for idx in indices]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "# 构建词汇表\n",
    "vocab = Vocabulary()\n",
    "vocab.build_vocab(cleaned_texts)\n",
    "\n",
    "print(f\"词汇表大小: {len(vocab)}\")\n",
    "print(f\"词汇表: {vocab.word2idx}\")\n",
    "\n",
    "# 编码示例\n",
    "encoded = vocab.encode(cleaned_texts[0])\n",
    "print(f\"\\n原文: {cleaned_texts[0]}\")\n",
    "print(f\"编码: {encoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填充序列\n",
    "def pad_sequences(sequences, max_len, pad_value=0):\n",
    "    padded = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) > max_len:\n",
    "            padded.append(seq[:max_len])\n",
    "        else:\n",
    "            padded.append(seq + [pad_value] * (max_len - len(seq)))\n",
    "    return padded\n",
    "\n",
    "# 编码所有文本\n",
    "encoded_texts = [vocab.encode(tokens) for tokens in cleaned_texts]\n",
    "max_len = max(len(seq) for seq in encoded_texts)\n",
    "padded_texts = pad_sequences(encoded_texts, max_len)\n",
    "\n",
    "print(f\"最大长度: {max_len}\")\n",
    "print(f\"填充后的序列:\")\n",
    "for seq in padded_texts:\n",
    "    print(f\"  {seq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 词嵌入（Word Embeddings）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词嵌入层\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 64\n",
    "\n",
    "embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "\n",
    "# 嵌入示例\n",
    "sample_input = torch.LongTensor([padded_texts[0]])\n",
    "embedded = embedding(sample_input)\n",
    "\n",
    "print(f\"输入形状: {sample_input.shape}\")\n",
    "print(f\"嵌入后形状: {embedded.shape}\")\n",
    "print(f\"\\n输入: {sample_input[0]}\")\n",
    "print(f\"嵌入向量 (前3个词的前5维):\")\n",
    "print(embedded[0, :3, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化词嵌入\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 获取所有词的嵌入\n",
    "all_words = list(vocab.word2idx.keys())\n",
    "all_indices = torch.LongTensor([vocab.word2idx[w] for w in all_words])\n",
    "all_embeddings = embedding(all_indices).detach().numpy()\n",
    "\n",
    "# PCA 降维\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(all_embeddings)\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.7)\n",
    "\n",
    "for i, word in enumerate(all_words):\n",
    "    plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]))\n",
    "\n",
    "plt.title('词嵌入可视化 (PCA)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 文本分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本分类数据集\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = clean_text(self.texts[idx])\n",
    "        encoded = self.vocab.encode(tokens)\n",
    "        \n",
    "        # 填充或截断\n",
    "        if len(encoded) > self.max_len:\n",
    "            encoded = encoded[:self.max_len]\n",
    "        else:\n",
    "            encoded = encoded + [0] * (self.max_len - len(encoded))\n",
    "        \n",
    "        return torch.LongTensor(encoded), torch.LongTensor([self.labels[idx]])\n",
    "\n",
    "# 创建数据集\n",
    "dataset = TextDataset(texts, labels, vocab, max_len)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 文本分类器\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, num_layers=2):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, \n",
    "                           batch_first=True, bidirectional=True, dropout=0.3)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len)\n",
    "        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
    "        \n",
    "        lstm_out, (h_n, c_n) = self.lstm(embedded)\n",
    "        # h_n: (num_layers*2, batch, hidden_dim)\n",
    "        \n",
    "        # 连接前向和后向的最后隐藏状态\n",
    "        hidden = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        \n",
    "        output = self.dropout(hidden)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "# 创建模型\n",
    "model = TextClassifier(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=64,\n",
    "    hidden_dim=128,\n",
    "    num_classes=2\n",
    ").to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 文本分类器\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes, num_filters=100):\n",
    "        super(TextCNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        # 多个不同大小的卷积核\n",
    "        self.conv1 = nn.Conv1d(embed_dim, num_filters, kernel_size=2)\n",
    "        self.conv2 = nn.Conv1d(embed_dim, num_filters, kernel_size=3)\n",
    "        self.conv3 = nn.Conv1d(embed_dim, num_filters, kernel_size=4)\n",
    "        \n",
    "        self.fc = nn.Linear(num_filters * 3, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len)\n",
    "        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
    "        embedded = embedded.permute(0, 2, 1)  # (batch, embed_dim, seq_len)\n",
    "        \n",
    "        # 卷积 + ReLU + 全局最大池化\n",
    "        c1 = F.relu(self.conv1(embedded))\n",
    "        c1 = F.max_pool1d(c1, c1.size(2)).squeeze(2)\n",
    "        \n",
    "        c2 = F.relu(self.conv2(embedded))\n",
    "        c2 = F.max_pool1d(c2, c2.size(2)).squeeze(2)\n",
    "        \n",
    "        c3 = F.relu(self.conv3(embedded))\n",
    "        c3 = F.max_pool1d(c3, c3.size(2)).squeeze(2)\n",
    "        \n",
    "        # 连接\n",
    "        concat = torch.cat((c1, c2, c3), dim=1)\n",
    "        output = self.dropout(concat)\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# 创建 TextCNN 模型\n",
    "cnn_model = TextCNN(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=64,\n",
    "    num_classes=2\n",
    ").to(device)\n",
    "\n",
    "print(cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 更大的数据集示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成更多训练数据\n",
    "positive_templates = [\n",
    "    \"I love this {}\",\n",
    "    \"This {} is amazing\",\n",
    "    \"Great {} highly recommend\",\n",
    "    \"Best {} ever\",\n",
    "    \"Wonderful {} experience\",\n",
    "    \"Fantastic {} will buy again\"\n",
    "]\n",
    "\n",
    "negative_templates = [\n",
    "    \"I hate this {}\",\n",
    "    \"This {} is terrible\",\n",
    "    \"Awful {} waste of money\",\n",
    "    \"Worst {} ever\",\n",
    "    \"Horrible {} experience\",\n",
    "    \"Bad {} never again\"\n",
    "]\n",
    "\n",
    "items = ['product', 'movie', 'book', 'service', 'food', 'app']\n",
    "\n",
    "# 生成数据\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "\n",
    "for template in positive_templates:\n",
    "    for item in items:\n",
    "        train_texts.append(template.format(item))\n",
    "        train_labels.append(1)\n",
    "\n",
    "for template in negative_templates:\n",
    "    for item in items:\n",
    "        train_texts.append(template.format(item))\n",
    "        train_labels.append(0)\n",
    "\n",
    "print(f\"训练数据量: {len(train_texts)}\")\n",
    "print(f\"正样本: {sum(train_labels)}, 负样本: {len(train_labels) - sum(train_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新构建词汇表\n",
    "cleaned_train = [clean_text(t) for t in train_texts]\n",
    "vocab = Vocabulary()\n",
    "vocab.build_vocab(cleaned_train)\n",
    "\n",
    "print(f\"词汇表大小: {len(vocab)}\")\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "train_dataset = TextDataset(train_texts, train_labels, vocab, max_len=10)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "model = TextClassifier(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=64,\n",
    "    hidden_dim=64,\n",
    "    num_classes=2\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.squeeze().to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_x)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == batch_y).sum().item()\n",
    "        total += batch_y.size(0)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}, Acc: {correct/total:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试模型\n",
    "def predict(model, text, vocab, max_len):\n",
    "    model.eval()\n",
    "    tokens = clean_text(text)\n",
    "    encoded = vocab.encode(tokens)\n",
    "    \n",
    "    if len(encoded) > max_len:\n",
    "        encoded = encoded[:max_len]\n",
    "    else:\n",
    "        encoded = encoded + [0] * (max_len - len(encoded))\n",
    "    \n",
    "    x = torch.LongTensor([encoded]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(x)\n",
    "        prob = F.softmax(output, dim=1)\n",
    "        pred = output.argmax(dim=1).item()\n",
    "    \n",
    "    return pred, prob[0].cpu().numpy()\n",
    "\n",
    "# 测试\n",
    "test_texts = [\n",
    "    \"I love this product\",\n",
    "    \"This service is terrible\",\n",
    "    \"Amazing experience\",\n",
    "    \"Horrible waste of time\"\n",
    "]\n",
    "\n",
    "print(\"预测结果:\")\n",
    "for text in test_texts:\n",
    "    pred, prob = predict(model, text, vocab, max_len=10)\n",
    "    sentiment = \"正面\" if pred == 1 else \"负面\"\n",
    "    print(f\"  '{text}'\")\n",
    "    print(f\"    预测: {sentiment}, 概率: 负面={prob[0]:.3f}, 正面={prob[1]:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 常见 NLP 任务概览"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "常见 NLP 任务:\n",
    "\n",
    "1. 文本分类\n",
    "   - 情感分析\n",
    "   - 垃圾邮件检测\n",
    "   - 主题分类\n",
    "\n",
    "2. 序列标注\n",
    "   - 命名实体识别 (NER)\n",
    "   - 词性标注 (POS)\n",
    "   - 分词\n",
    "\n",
    "3. 文本生成\n",
    "   - 机器翻译\n",
    "   - 文本摘要\n",
    "   - 对话系统\n",
    "\n",
    "4. 语义理解\n",
    "   - 问答系统\n",
    "   - 阅读理解\n",
    "   - 文本蕴含\n",
    "\n",
    "5. 信息抽取\n",
    "   - 关系抽取\n",
    "   - 事件抽取\n",
    "   - 知识图谱构建\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 练习题\n",
    "\n",
    "### 练习：改进文本分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在这里编写代码\n",
    "# 1. 添加更多训练数据\n",
    "# 2. 尝试使用 TextCNN 模型\n",
    "# 3. 添加注意力机制\n",
    "# 4. 比较不同模型的性能\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 本课小结\n",
    "\n",
    "1. **文本预处理**：清洗、分词、编码、填充\n",
    "2. **词汇表**：word2idx、idx2word 映射\n",
    "3. **词嵌入**：将离散词转为连续向量\n",
    "4. **模型**：LSTM、TextCNN 用于分类\n",
    "5. **常见任务**：分类、标注、生成、理解"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
