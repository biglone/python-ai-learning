{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第9课：强化学习入门\n",
    "\n",
    "## 学习目标\n",
    "- 理解强化学习的基本概念\n",
    "- 掌握 Q-Learning 算法\n",
    "- 实现简单的强化学习环境\n",
    "- 了解深度强化学习 (DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 强化学习基本概念\n",
    "\n",
    "强化学习是一种通过与环境交互来学习的方法。\n",
    "\n",
    "### 核心要素\n",
    "\n",
    "- **Agent (智能体)**：学习和做决策的主体\n",
    "- **Environment (环境)**：智能体交互的外部世界\n",
    "- **State (状态)**：环境的当前情况\n",
    "- **Action (动作)**：智能体可以采取的行为\n",
    "- **Reward (奖励)**：环境对动作的反馈\n",
    "- **Policy (策略)**：从状态到动作的映射\n",
    "\n",
    "### 学习目标\n",
    "\n",
    "最大化累积奖励：$R = \\sum_{t=0}^{\\infty} \\gamma^t r_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化强化学习循环\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Agent\n",
    "agent_circle = plt.Circle((0.2, 0.5), 0.1, color='lightblue', ec='black')\n",
    "ax.add_patch(agent_circle)\n",
    "ax.text(0.2, 0.5, 'Agent', ha='center', va='center', fontsize=12)\n",
    "\n",
    "# Environment\n",
    "env_rect = plt.Rectangle((0.6, 0.3), 0.3, 0.4, color='lightgreen', ec='black')\n",
    "ax.add_patch(env_rect)\n",
    "ax.text(0.75, 0.5, 'Environment', ha='center', va='center', fontsize=12)\n",
    "\n",
    "# Arrows\n",
    "ax.annotate('', xy=(0.6, 0.6), xytext=(0.3, 0.6),\n",
    "            arrowprops=dict(arrowstyle='->', color='blue', lw=2))\n",
    "ax.text(0.45, 0.65, 'Action', ha='center', fontsize=10, color='blue')\n",
    "\n",
    "ax.annotate('', xy=(0.3, 0.4), xytext=(0.6, 0.4),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "ax.text(0.45, 0.35, 'State, Reward', ha='center', fontsize=10, color='red')\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axis('off')\n",
    "ax.set_title('Reinforcement Learning Loop', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 网格世界环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \"\"\"简单的网格世界环境\"\"\"\n",
    "    \n",
    "    def __init__(self, size=4):\n",
    "        self.size = size\n",
    "        self.start = (0, 0)\n",
    "        self.goal = (size-1, size-1)\n",
    "        self.obstacles = [(1, 1), (2, 2)]  # 障碍物\n",
    "        self.state = self.start\n",
    "        \n",
    "        # 动作: 0=上, 1=下, 2=左, 3=右\n",
    "        self.actions = [0, 1, 2, 3]\n",
    "        self.action_effects = {\n",
    "            0: (-1, 0),  # 上\n",
    "            1: (1, 0),   # 下\n",
    "            2: (0, -1),  # 左\n",
    "            3: (0, 1)    # 右\n",
    "        }\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"重置环境\"\"\"\n",
    "        self.state = self.start\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"执行动作\"\"\"\n",
    "        # 计算新位置\n",
    "        dr, dc = self.action_effects[action]\n",
    "        new_row = self.state[0] + dr\n",
    "        new_col = self.state[1] + dc\n",
    "        \n",
    "        # 检查边界\n",
    "        if 0 <= new_row < self.size and 0 <= new_col < self.size:\n",
    "            new_state = (new_row, new_col)\n",
    "            # 检查障碍物\n",
    "            if new_state not in self.obstacles:\n",
    "                self.state = new_state\n",
    "        \n",
    "        # 计算奖励\n",
    "        if self.state == self.goal:\n",
    "            reward = 10\n",
    "            done = True\n",
    "        elif self.state in self.obstacles:\n",
    "            reward = -5\n",
    "            done = False\n",
    "        else:\n",
    "            reward = -1  # 每步小惩罚，鼓励快速到达目标\n",
    "            done = False\n",
    "        \n",
    "        return self.state, reward, done\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"可视化当前状态\"\"\"\n",
    "        grid = np.zeros((self.size, self.size))\n",
    "        \n",
    "        # 标记障碍物\n",
    "        for obs in self.obstacles:\n",
    "            grid[obs] = -1\n",
    "        \n",
    "        # 标记目标\n",
    "        grid[self.goal] = 2\n",
    "        \n",
    "        # 标记智能体\n",
    "        grid[self.state] = 1\n",
    "        \n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(grid, cmap='RdYlGn')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                if (i, j) == self.state:\n",
    "                    plt.text(j, i, 'A', ha='center', va='center', fontsize=20)\n",
    "                elif (i, j) == self.goal:\n",
    "                    plt.text(j, i, 'G', ha='center', va='center', fontsize=20)\n",
    "                elif (i, j) in self.obstacles:\n",
    "                    plt.text(j, i, 'X', ha='center', va='center', fontsize=20)\n",
    "        \n",
    "        plt.title('Grid World')\n",
    "        plt.show()\n",
    "\n",
    "# 测试环境\n",
    "env = GridWorld(size=4)\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Q-Learning 算法\n",
    "\n",
    "Q-Learning 是一种经典的强化学习算法，学习状态-动作值函数 Q(s, a)。\n",
    "\n",
    "### Q 值更新公式\n",
    "\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)]$$\n",
    "\n",
    "其中：\n",
    "- $\\alpha$: 学习率\n",
    "- $\\gamma$: 折扣因子\n",
    "- $r$: 即时奖励"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    \"\"\"Q-Learning 算法\"\"\"\n",
    "    \n",
    "    def __init__(self, n_actions, learning_rate=0.1, gamma=0.99, epsilon=0.1):\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Q 表\n",
    "        self.q_table = defaultdict(lambda: np.zeros(n_actions))\n",
    "    \n",
    "    def choose_action(self, state, training=True):\n",
    "        \"\"\"ε-贪婪策略选择动作\"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randint(0, self.n_actions - 1)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"更新 Q 值\"\"\"\n",
    "        current_q = self.q_table[state][action]\n",
    "        \n",
    "        if done:\n",
    "            target_q = reward\n",
    "        else:\n",
    "            target_q = reward + self.gamma * np.max(self.q_table[next_state])\n",
    "        \n",
    "        # Q 值更新\n",
    "        self.q_table[state][action] += self.lr * (target_q - current_q)\n",
    "    \n",
    "    def get_policy(self):\n",
    "        \"\"\"获取当前策略\"\"\"\n",
    "        policy = {}\n",
    "        for state, q_values in self.q_table.items():\n",
    "            policy[state] = np.argmax(q_values)\n",
    "        return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_qlearning(env, agent, n_episodes=1000):\n",
    "    \"\"\"训练 Q-Learning 智能体\"\"\"\n",
    "    rewards_history = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # 选择动作\n",
    "            action = agent.choose_action(state)\n",
    "            \n",
    "            # 执行动作\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # 更新 Q 值\n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        rewards_history.append(total_reward)\n",
    "        \n",
    "        if (episode + 1) % 200 == 0:\n",
    "            avg_reward = np.mean(rewards_history[-100:])\n",
    "            print(f'Episode {episode+1}, Average Reward (last 100): {avg_reward:.2f}')\n",
    "    \n",
    "    return rewards_history\n",
    "\n",
    "# 训练\n",
    "env = GridWorld(size=4)\n",
    "agent = QLearning(n_actions=4, learning_rate=0.1, gamma=0.99, epsilon=0.2)\n",
    "\n",
    "rewards = train_qlearning(env, agent, n_episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制学习曲线\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rewards, alpha=0.3)\n",
    "window = 50\n",
    "smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "plt.plot(range(window-1, len(rewards)), smoothed, color='red', linewidth=2)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Learning Curve')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 显示 Q 表热图\n",
    "plt.subplot(1, 2, 2)\n",
    "q_values = np.zeros((4, 4, 4))  # 状态 x 动作\n",
    "for state, values in agent.q_table.items():\n",
    "    q_values[state[0], state[1]] = values\n",
    "\n",
    "# 显示最大 Q 值\n",
    "max_q = np.max(q_values, axis=2)\n",
    "plt.imshow(max_q, cmap='viridis')\n",
    "plt.colorbar(label='Max Q Value')\n",
    "plt.title('Learned Value Function')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化学习到的策略\n",
    "def visualize_policy(env, agent):\n",
    "    \"\"\"可视化学习到的策略\"\"\"\n",
    "    action_symbols = ['↑', '↓', '←', '→']\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    \n",
    "    for i in range(env.size):\n",
    "        for j in range(env.size):\n",
    "            state = (i, j)\n",
    "            if state == env.goal:\n",
    "                plt.text(j, i, 'G', ha='center', va='center', fontsize=20, color='green')\n",
    "            elif state in env.obstacles:\n",
    "                plt.text(j, i, 'X', ha='center', va='center', fontsize=20, color='red')\n",
    "            else:\n",
    "                action = agent.choose_action(state, training=False)\n",
    "                plt.text(j, i, action_symbols[action], ha='center', va='center', fontsize=20)\n",
    "    \n",
    "    plt.xlim(-0.5, env.size - 0.5)\n",
    "    plt.ylim(env.size - 0.5, -0.5)\n",
    "    plt.grid(True)\n",
    "    plt.title('Learned Policy')\n",
    "    plt.show()\n",
    "\n",
    "visualize_policy(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试学习到的策略\n",
    "def test_policy(env, agent, n_episodes=10):\n",
    "    \"\"\"测试学习到的策略\"\"\"\n",
    "    total_rewards = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        \n",
    "        while not done and steps < 50:\n",
    "            action = agent.choose_action(state, training=False)\n",
    "            state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "        print(f'Episode {episode+1}: Reward={total_reward}, Steps={steps}, Success={done}')\n",
    "    \n",
    "    print(f'\\nAverage Reward: {np.mean(total_rewards):.2f}')\n",
    "    print(f'Success Rate: {sum(r > 0 for r in total_rewards) / n_episodes * 100:.1f}%')\n",
    "\n",
    "test_policy(env, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 使用 Gymnasium\n",
    "\n",
    "Gymnasium (原 OpenAI Gym) 是强化学习的标准环境库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 安装: pip install gymnasium\ntry:\n    import gymnasium as gym\n    GYMNASIUM_AVAILABLE = True\n    print(\"Gymnasium 已安装\")\nexcept ImportError:\n    GYMNASIUM_AVAILABLE = False\n    print(\"请先安装: pip install gymnasium\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if GYMNASIUM_AVAILABLE:\n    import gymnasium as gym\n\n    # 创建 FrozenLake 环境\n    env_gym = gym.make('FrozenLake-v1', is_slippery=False)\n\n    print(f\"状态空间: {env_gym.observation_space}\")\n    print(f\"动作空间: {env_gym.action_space}\")\n    print(f\"动作含义: 0=左, 1=下, 2=右, 3=上\")\nelse:\n    print(\"跳过 Gymnasium 环境（未安装）\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if GYMNASIUM_AVAILABLE:\n    # 在 Gymnasium 环境上训练\n    def train_on_gym(env, n_episodes=2000):\n        agent = QLearning(n_actions=env.action_space.n, \n                          learning_rate=0.8, gamma=0.95, epsilon=0.1)\n        rewards_history = []\n        \n        for episode in range(n_episodes):\n            state, _ = env.reset()\n            total_reward = 0\n            done = False\n            truncated = False\n            \n            while not done and not truncated:\n                action = agent.choose_action(state)\n                next_state, reward, done, truncated, _ = env.step(action)\n                agent.update(state, action, reward, next_state, done)\n                state = next_state\n                total_reward += reward\n            \n            rewards_history.append(total_reward)\n            \n            if (episode + 1) % 500 == 0:\n                success_rate = np.mean(rewards_history[-100:]) * 100\n                print(f'Episode {episode+1}, Success Rate: {success_rate:.1f}%')\n        \n        return agent, rewards_history\n\n    agent_gym, rewards_gym = train_on_gym(env_gym)\nelse:\n    print(\"跳过 Gymnasium 训练（未安装）\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Deep Q-Network (DQN) 简介\n",
    "\n",
    "当状态空间很大时，Q 表不再适用，需要用神经网络来近似 Q 函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"Deep Q-Network\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"经验回放缓冲区\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (np.array(states), np.array(actions), \n",
    "                np.array(rewards), np.array(next_states), np.array(dones))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "print(\"DQN 网络结构:\")\n",
    "print(DQN(state_dim=4, action_dim=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"DQN 智能体\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99, epsilon=1.0):\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        \n",
    "        # Q 网络\n",
    "        self.q_network = DQN(state_dim, action_dim)\n",
    "        self.target_network = DQN(state_dim, action_dim)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.buffer = ReplayBuffer()\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        \n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(state_tensor)\n",
    "        return q_values.argmax().item()\n",
    "    \n",
    "    def train(self, batch_size=32):\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample(batch_size)\n",
    "        \n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "        \n",
    "        # 计算当前 Q 值\n",
    "        current_q = self.q_network(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # 计算目标 Q 值\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_network(next_states).max(1)[0]\n",
    "            target_q = rewards + (1 - dones) * self.gamma * next_q\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = nn.MSELoss()(current_q.squeeze(), target_q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # 衰减 epsilon\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "    \n",
    "    def update_target(self):\n",
    "        \"\"\"更新目标网络\"\"\"\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if GYMNASIUM_AVAILABLE:\n    # 在 CartPole 环境测试 DQN\n    env_cartpole = gym.make('CartPole-v1')\n\n    state_dim = env_cartpole.observation_space.shape[0]\n    action_dim = env_cartpole.action_space.n\n\n    print(f\"状态维度: {state_dim}\")\n    print(f\"动作数量: {action_dim}\")\nelse:\n    print(\"跳过 CartPole 环境（未安装 Gymnasium）\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if GYMNASIUM_AVAILABLE:\n    def train_dqn(env, agent, n_episodes=500):\n        rewards_history = []\n        \n        for episode in range(n_episodes):\n            state, _ = env.reset()\n            total_reward = 0\n            done = False\n            truncated = False\n            \n            while not done and not truncated:\n                action = agent.choose_action(state)\n                next_state, reward, done, truncated, _ = env.step(action)\n                \n                agent.buffer.push(state, action, reward, next_state, done)\n                agent.train()\n                \n                state = next_state\n                total_reward += reward\n            \n            # 定期更新目标网络\n            if episode % 10 == 0:\n                agent.update_target()\n            \n            rewards_history.append(total_reward)\n            \n            if (episode + 1) % 50 == 0:\n                avg_reward = np.mean(rewards_history[-50:])\n                print(f'Episode {episode+1}, Avg Reward: {avg_reward:.1f}, Epsilon: {agent.epsilon:.3f}')\n        \n        return rewards_history\n\n    # 训练 (减少 episode 数用于演示)\n    dqn_agent = DQNAgent(state_dim, action_dim)\n    dqn_rewards = train_dqn(env_cartpole, dqn_agent, n_episodes=200)\nelse:\n    print(\"跳过 DQN 训练（未安装 Gymnasium）\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 练习题\n",
    "\n",
    "### 练习1：改进 Q-Learning\n",
    "实现 ε 衰减策略，让智能体逐渐减少探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在这里编写代码\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习2：扩展网格世界\n",
    "增加更多障碍物和奖励，观察学习效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在这里编写代码\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 本课小结\n",
    "\n",
    "### 强化学习核心概念\n",
    "\n",
    "1. **状态 (State)**：环境的观测\n",
    "2. **动作 (Action)**：智能体的行为\n",
    "3. **奖励 (Reward)**：即时反馈\n",
    "4. **策略 (Policy)**：状态到动作的映射\n",
    "5. **价值函数**：长期奖励的期望\n",
    "\n",
    "### 算法对比\n",
    "\n",
    "| 算法 | 状态空间 | 优点 | 缺点 |\n",
    "|------|----------|------|------|\n",
    "| Q-Learning | 离散小规模 | 简单易实现 | 不适合大状态空间 |\n",
    "| DQN | 连续/大规模 | 处理复杂环境 | 训练不稳定 |\n",
    "| Policy Gradient | 连续动作 | 直接优化策略 | 高方差 |\n",
    "\n",
    "### 进一步学习\n",
    "\n",
    "1. **Policy Gradient 方法**：REINFORCE、A2C\n",
    "2. **Actor-Critic 方法**：A3C、PPO、SAC\n",
    "3. **模型学习**：World Models、Dreamer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}