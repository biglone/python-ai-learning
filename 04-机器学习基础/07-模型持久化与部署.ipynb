{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第7课：模型持久化与部署\n",
    "\n",
    "## 学习目标\n",
    "- 掌握模型保存和加载方法\n",
    "- 了解模型部署的基本流程\n",
    "- 学会使用 Flask 创建 API\n",
    "- 了解模型监控和维护"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 模型持久化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import joblib\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 加载数据并训练模型\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris.data, iris.target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 创建流水线\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(f\"模型训练完成，测试准确率: {pipeline.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 使用 pickle 保存模型\n",
    "with open('model_pickle.pkl', 'wb') as f:\n",
    "    pickle.dump(pipeline, f)\n",
    "\n",
    "# 加载模型\n",
    "with open('model_pickle.pkl', 'rb') as f:\n",
    "    loaded_model_pickle = pickle.load(f)\n",
    "\n",
    "print(f\"Pickle 加载模型准确率: {loaded_model_pickle.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 使用 joblib 保存模型（推荐用于大模型）\n",
    "joblib.dump(pipeline, 'model_joblib.pkl')\n",
    "\n",
    "# 加载模型\n",
    "loaded_model_joblib = joblib.load('model_joblib.pkl')\n",
    "\n",
    "print(f\"Joblib 加载模型准确率: {loaded_model_joblib.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 1.3 保存模型元数据\nimport json\nfrom datetime import datetime\n\nmodel_metadata = {\n    'model_name': 'iris_classifier',\n    'model_type': 'RandomForestClassifier',\n    'version': '1.0.0',\n    'training_date': datetime.now().isoformat(),\n    'features': iris.feature_names.tolist(),\n    'target_names': iris.target_names.tolist(),\n    'accuracy': float(pipeline.score(X_test, y_test)),\n    'hyperparameters': {\n        'n_estimators': 100,\n        'random_state': 42\n    }\n}\n\nwith open('model_metadata.json', 'w', encoding='utf-8') as f:\n    json.dump(model_metadata, f, indent=2, ensure_ascii=False)\n\nprint(\"模型元数据:\")\nprint(json.dumps(model_metadata, indent=2, ensure_ascii=False))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 模型版本管理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom datetime import datetime\n\nclass ModelVersionManager:\n    def __init__(self, base_path='models'):\n        self.base_path = base_path\n        os.makedirs(base_path, exist_ok=True)\n    \n    def save_model(self, model, model_name, metrics=None):\n        \"\"\"保存模型并创建版本\"\"\"\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        version_path = os.path.join(self.base_path, model_name, timestamp)\n        os.makedirs(version_path, exist_ok=True)\n        \n        # 保存模型\n        model_path = os.path.join(version_path, 'model.pkl')\n        joblib.dump(model, model_path)\n        \n        # 保存元数据\n        metadata = {\n            'model_name': model_name,\n            'version': timestamp,\n            'created_at': datetime.now().isoformat(),\n            'metrics': metrics or {}\n        }\n        metadata_path = os.path.join(version_path, 'metadata.json')\n        with open(metadata_path, 'w', encoding='utf-8') as f:\n            json.dump(metadata, f, indent=2, ensure_ascii=False)\n        \n        print(f\"模型已保存: {version_path}\")\n        return version_path\n    \n    def load_model(self, model_name, version='latest'):\n        \"\"\"加载指定版本的模型\"\"\"\n        model_dir = os.path.join(self.base_path, model_name)\n        \n        if version == 'latest':\n            versions = sorted(os.listdir(model_dir))\n            version = versions[-1]\n        \n        model_path = os.path.join(model_dir, version, 'model.pkl')\n        return joblib.load(model_path)\n    \n    def list_versions(self, model_name):\n        \"\"\"列出所有版本\"\"\"\n        model_dir = os.path.join(self.base_path, model_name)\n        if os.path.exists(model_dir):\n            return sorted(os.listdir(model_dir))\n        return []\n\n# 使用示例\nmanager = ModelVersionManager()\n\n# 保存模型\nmetrics = {'accuracy': 0.95, 'f1_score': 0.94}\nmanager.save_model(pipeline, 'iris_classifier', metrics)\n\n# 列出版本\nprint(f\"\\n可用版本: {manager.list_versions('iris_classifier')}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 创建预测 API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 Flask API（保存为 app.py）\n",
    "flask_code = '''\n",
    "from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# 加载模型\n",
    "model = joblib.load(\"model_joblib.pkl\")\n",
    "\n",
    "# 特征名和类别名\n",
    "feature_names = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "target_names = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "\n",
    "@app.route(\"/health\", methods=[\"GET\"])\n",
    "def health():\n",
    "    return jsonify({\"status\": \"healthy\"})\n",
    "\n",
    "@app.route(\"/predict\", methods=[\"POST\"])\n",
    "def predict():\n",
    "    try:\n",
    "        data = request.json\n",
    "        features = np.array([[data[f] for f in feature_names]])\n",
    "        \n",
    "        prediction = model.predict(features)[0]\n",
    "        probability = model.predict_proba(features)[0].tolist()\n",
    "        \n",
    "        return jsonify({\n",
    "            \"prediction\": target_names[prediction],\n",
    "            \"prediction_id\": int(prediction),\n",
    "            \"probabilities\": dict(zip(target_names, probability))\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 400\n",
    "\n",
    "@app.route(\"/batch_predict\", methods=[\"POST\"])\n",
    "def batch_predict():\n",
    "    try:\n",
    "        data = request.json\n",
    "        samples = data[\"samples\"]\n",
    "        features = np.array([[s[f] for f in feature_names] for s in samples])\n",
    "        \n",
    "        predictions = model.predict(features)\n",
    "        \n",
    "        results = [\n",
    "            {\"prediction\": target_names[p], \"prediction_id\": int(p)}\n",
    "            for p in predictions\n",
    "        ]\n",
    "        \n",
    "        return jsonify({\"predictions\": results})\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 400\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=5000, debug=True)\n",
    "'''\n",
    "\n",
    "print(\"Flask API 代码示例:\")\n",
    "print(flask_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 保存 Flask 应用代码\nwith open('app.py', 'w', encoding='utf-8') as f:\n    f.write(flask_code)\n\nprint(\"Flask 应用已保存到 app.py\")\nprint(\"\\n运行命令: python app.py\")\nprint(\"\\n测试 API:\")\nprint('''curl -X POST http://localhost:5000/predict \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"sepal_length\": 5.1, \"sepal_width\": 3.5, \"petal_length\": 1.4, \"petal_width\": 0.2}' ''')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 使用 FastAPI（现代方案）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fastapi_code = '''\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List\nimport joblib\nimport numpy as np\n\napp = FastAPI(title=\"Iris Classifier API\", version=\"1.0.0\")\n\n# 加载模型\nmodel = joblib.load(\"model_joblib.pkl\")\ntarget_names = [\"setosa\", \"versicolor\", \"virginica\"]\n\n# 请求模型\nclass IrisFeatures(BaseModel):\n    sepal_length: float\n    sepal_width: float\n    petal_length: float\n    petal_width: float\n\nclass BatchRequest(BaseModel):\n    samples: List[IrisFeatures]\n\n# 响应模型\nclass PredictionResponse(BaseModel):\n    prediction: str\n    prediction_id: int\n    probabilities: dict\n\n@app.get(\"/health\")\ndef health():\n    return {\"status\": \"healthy\"}\n\n@app.post(\"/predict\", response_model=PredictionResponse)\ndef predict(features: IrisFeatures):\n    try:\n        X = np.array([[features.sepal_length, features.sepal_width,\n                       features.petal_length, features.petal_width]])\n        \n        prediction = model.predict(X)[0]\n        probability = model.predict_proba(X)[0].tolist()\n        \n        return {\n            \"prediction\": target_names[prediction],\n            \"prediction_id\": int(prediction),\n            \"probabilities\": dict(zip(target_names, probability))\n        }\n    except Exception as e:\n        raise HTTPException(status_code=400, detail=str(e))\n\n@app.post(\"/batch_predict\")\ndef batch_predict(batch: BatchRequest):\n    try:\n        X = np.array([[s.sepal_length, s.sepal_width, \n                       s.petal_length, s.petal_width] \n                      for s in batch.samples])\n        \n        predictions = model.predict(X)\n        \n        return {\n            \"predictions\": [\n                {\"prediction\": target_names[p], \"prediction_id\": int(p)}\n                for p in predictions\n            ]\n        }\n    except Exception as e:\n        raise HTTPException(status_code=400, detail=str(e))\n\n# 运行: uvicorn fastapi_app:app --reload\n'''\n\nwith open('fastapi_app.py', 'w', encoding='utf-8') as f:\n    f.write(fastapi_code)\n\nprint(\"FastAPI 应用已保存到 fastapi_app.py\")\nprint(\"\\n运行命令: uvicorn fastapi_app:app --reload\")\nprint(\"API 文档: http://localhost:8000/docs\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Docker 容器化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Dockerfile\ndockerfile = '''\nFROM python:3.10-slim\n\nWORKDIR /app\n\n# 安装依赖\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# 复制代码和模型\nCOPY app.py .\nCOPY model_joblib.pkl .\n\n# 暴露端口\nEXPOSE 5000\n\n# 启动应用\nCMD [\"python\", \"app.py\"]\n'''\n\n# requirements.txt\nrequirements = '''\nflask==2.3.0\nscikit-learn==1.3.0\njoblib==1.3.0\nnumpy==1.24.0\n'''\n\nprint(\"Dockerfile:\")\nprint(dockerfile)\nprint(\"\\nrequirements.txt:\")\nprint(requirements)\n\n# 保存文件\nwith open('Dockerfile', 'w', encoding='utf-8') as f:\n    f.write(dockerfile)\n\nwith open('requirements.txt', 'w', encoding='utf-8') as f:\n    f.write(requirements)\n\nprint(\"\\n构建 Docker 镜像: docker build -t iris-classifier .\")\nprint(\"运行容器: docker run -p 5000:5000 iris-classifier\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 模型监控"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "class ModelMonitor:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.predictions = []\n",
    "        \n",
    "        # 设置日志\n",
    "        logging.basicConfig(\n",
    "            filename=f'{model_name}_predictions.log',\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(message)s'\n",
    "        )\n",
    "    \n",
    "    def log_prediction(self, input_features, prediction, probability=None):\n",
    "        \"\"\"记录预测\"\"\"\n",
    "        record = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'input': input_features.tolist() if hasattr(input_features, 'tolist') else input_features,\n",
    "            'prediction': prediction,\n",
    "            'probability': probability\n",
    "        }\n",
    "        self.predictions.append(record)\n",
    "        logging.info(json.dumps(record))\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        \"\"\"获取预测统计\"\"\"\n",
    "        if not self.predictions:\n",
    "            return {}\n",
    "        \n",
    "        predictions_list = [p['prediction'] for p in self.predictions]\n",
    "        from collections import Counter\n",
    "        counts = Counter(predictions_list)\n",
    "        \n",
    "        return {\n",
    "            'total_predictions': len(self.predictions),\n",
    "            'prediction_distribution': dict(counts),\n",
    "            'first_prediction': self.predictions[0]['timestamp'],\n",
    "            'last_prediction': self.predictions[-1]['timestamp']\n",
    "        }\n",
    "\n",
    "# 使用示例\n",
    "monitor = ModelMonitor('iris_classifier')\n",
    "\n",
    "# 模拟预测并记录\n",
    "for i in range(10):\n",
    "    X_sample = X_test[i:i+1]\n",
    "    pred = pipeline.predict(X_sample)[0]\n",
    "    prob = pipeline.predict_proba(X_sample)[0].tolist()\n",
    "    monitor.log_prediction(X_sample[0], iris.target_names[pred], prob)\n",
    "\n",
    "print(\"预测统计:\")\n",
    "print(json.dumps(monitor.get_statistics(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据漂移检测\n",
    "from scipy import stats\n",
    "\n",
    "def detect_drift(reference_data, current_data, threshold=0.05):\n",
    "    \"\"\"使用 KS 检验检测数据漂移\"\"\"\n",
    "    drift_results = {}\n",
    "    \n",
    "    for i, feature in enumerate(iris.feature_names):\n",
    "        ref = reference_data[:, i]\n",
    "        cur = current_data[:, i]\n",
    "        \n",
    "        statistic, p_value = stats.ks_2samp(ref, cur)\n",
    "        drift_results[feature] = {\n",
    "            'statistic': statistic,\n",
    "            'p_value': p_value,\n",
    "            'drift_detected': p_value < threshold\n",
    "        }\n",
    "    \n",
    "    return drift_results\n",
    "\n",
    "# 模拟新数据（带漂移）\n",
    "np.random.seed(123)\n",
    "X_new = X_test + np.random.normal(0.5, 0.1, X_test.shape)\n",
    "\n",
    "drift_results = detect_drift(X_train, X_new)\n",
    "print(\"数据漂移检测结果:\")\n",
    "for feature, result in drift_results.items():\n",
    "    status = \"检测到漂移\" if result['drift_detected'] else \"正常\"\n",
    "    print(f\"  {feature}: p={result['p_value']:.4f} - {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 练习题\n",
    "\n",
    "### 练习：创建完整的模型部署流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在这里编写代码\n",
    "# 1. 训练一个分类模型\n",
    "# 2. 保存模型和元数据\n",
    "# 3. 创建预测函数\n",
    "# 4. 添加日志记录\n",
    "# 5. 实现简单的监控\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 本课小结\n",
    "\n",
    "1. **模型持久化**：pickle、joblib 保存和加载\n",
    "2. **版本管理**：记录模型版本和元数据\n",
    "3. **API 部署**：Flask、FastAPI 创建 REST API\n",
    "4. **容器化**：使用 Docker 打包部署\n",
    "5. **模型监控**：记录预测、检测数据漂移"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 清理临时文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清理示例中创建的文件（可选）\n",
    "import os\n",
    "\n",
    "files_to_clean = [\n",
    "    'model_pickle.pkl', 'model_joblib.pkl', 'model_metadata.json',\n",
    "    'app.py', 'fastapi_app.py', 'Dockerfile', 'requirements.txt',\n",
    "    'iris_classifier_predictions.log'\n",
    "]\n",
    "\n",
    "# 取消注释以下代码来清理文件\n",
    "# for f in files_to_clean:\n",
    "#     if os.path.exists(f):\n",
    "#         os.remove(f)\n",
    "#         print(f\"已删除: {f}\")\n",
    "\n",
    "print(\"提示：如需清理临时文件，请取消上方代码的注释\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}