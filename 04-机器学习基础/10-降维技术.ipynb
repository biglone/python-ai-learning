{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第10课：降维技术\n",
    "\n",
    "## 学习目标\n",
    "- 理解降维的意义和应用场景\n",
    "- 掌握 PCA 主成分分析\n",
    "- 掌握 t-SNE 可视化\n",
    "- 了解 UMAP 降维方法\n",
    "- 学习特征选择方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits, load_iris, fetch_olivetti_faces\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 为什么需要降维？\n",
    "\n",
    "**问题**：\n",
    "- 高维数据难以可视化\n",
    "- 维度灾难：高维空间数据稀疏\n",
    "- 特征冗余：很多特征相关性高\n",
    "- 计算成本高\n",
    "\n",
    "**降维的好处**：\n",
    "- 数据可视化\n",
    "- 降低计算成本\n",
    "- 去除噪声\n",
    "- 缓解过拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载手写数字数据集\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "print(f\"数据形状: {X.shape}\")\n",
    "print(f\"每个样本是 8x8 = 64 维的图像\")\n",
    "print(f\"类别数: {len(np.unique(y))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化部分样本\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X[i].reshape(8, 8), cmap='gray')\n",
    "    ax.set_title(f'Label: {y[i]}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Digit Samples')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PCA 主成分分析\n",
    "\n",
    "PCA (Principal Component Analysis) 是最常用的线性降维方法。\n",
    "\n",
    "**核心思想**：\n",
    "- 找到方差最大的方向（主成分）\n",
    "- 将数据投影到这些主成分上\n",
    "- 保留最重要的成分，丢弃次要的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准化\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# PCA 降维到 2D\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_pca_2d = pca_2d.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"原始维度: {X.shape[1]}\")\n",
    "print(f\"降维后维度: {X_pca_2d.shape[1]}\")\n",
    "print(f\"解释方差比例: {pca_2d.explained_variance_ratio_}\")\n",
    "print(f\"累计解释方差: {sum(pca_2d.explained_variance_ratio_):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化 PCA 结果\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "scatter = plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=y, cmap='tab10', alpha=0.7)\n",
    "plt.colorbar(scatter, label='Digit')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title('PCA Visualization of Digits Dataset (2D)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析需要多少个主成分\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled)\n",
    "\n",
    "# 累计解释方差\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 单个成分解释方差\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(1, 21), pca_full.explained_variance_ratio_[:20])\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Variance Explained by Each Component')\n",
    "\n",
    "# 累计解释方差\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(cumulative_variance)+1), cumulative_variance, 'b-o')\n",
    "plt.axhline(y=0.9, color='r', linestyle='--', label='90% variance')\n",
    "plt.axhline(y=0.95, color='g', linestyle='--', label='95% variance')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Explained Variance')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 找到保留 90% 和 95% 方差所需的成分数\n",
    "n_90 = np.argmax(cumulative_variance >= 0.9) + 1\n",
    "n_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "print(f\"保留 90% 方差需要 {n_90} 个成分\")\n",
    "print(f\"保留 95% 方差需要 {n_95} 个成分\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PCA 用于分类任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比较不同维度对分类效果的影响\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "n_components_list = [2, 5, 10, 20, 30, 40, 50, 64]\n",
    "accuracies = []\n",
    "\n",
    "for n in n_components_list:\n",
    "    if n < X.shape[1]:\n",
    "        pca = PCA(n_components=n)\n",
    "        X_train_pca = pca.fit_transform(X_train)\n",
    "        X_test_pca = pca.transform(X_test)\n",
    "    else:\n",
    "        X_train_pca = X_train\n",
    "        X_test_pca = X_test\n",
    "    \n",
    "    clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    clf.fit(X_train_pca, y_train)\n",
    "    acc = accuracy_score(y_test, clf.predict(X_test_pca))\n",
    "    accuracies.append(acc)\n",
    "    print(f\"n_components={n}: 准确率={acc:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(n_components_list, accuracies, 'b-o')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Classification Accuracy vs Number of PCA Components')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. t-SNE 可视化\n",
    "\n",
    "t-SNE (t-Distributed Stochastic Neighbor Embedding) 是一种非线性降维方法，特别适合数据可视化。\n",
    "\n",
    "**特点**：\n",
    "- 保持局部结构\n",
    "- 适合可视化聚类\n",
    "- 计算成本较高\n",
    "- 不适合用于新数据转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE 降维\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"t-SNE 输出形状: {X_tsne.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对比 PCA 和 t-SNE\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# PCA\n",
    "scatter1 = axes[0].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=y, cmap='tab10', alpha=0.7, s=10)\n",
    "axes[0].set_xlabel('Component 1')\n",
    "axes[0].set_ylabel('Component 2')\n",
    "axes[0].set_title('PCA')\n",
    "\n",
    "# t-SNE\n",
    "scatter2 = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', alpha=0.7, s=10)\n",
    "axes[1].set_xlabel('Component 1')\n",
    "axes[1].set_ylabel('Component 2')\n",
    "axes[1].set_title('t-SNE')\n",
    "\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Digit')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不同 perplexity 的影响\n",
    "perplexities = [5, 30, 50, 100]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "for idx, perp in enumerate(perplexities):\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=perp)\n",
    "    X_tsne_temp = tsne.fit_transform(X_scaled)\n",
    "    \n",
    "    axes[idx].scatter(X_tsne_temp[:, 0], X_tsne_temp[:, 1], c=y, cmap='tab10', alpha=0.7, s=10)\n",
    "    axes[idx].set_title(f'perplexity = {perp}')\n",
    "    axes[idx].set_xlabel('Component 1')\n",
    "    axes[idx].set_ylabel('Component 2')\n",
    "\n",
    "plt.suptitle('t-SNE with Different Perplexity Values')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. UMAP 降维\n",
    "\n",
    "UMAP (Uniform Manifold Approximation and Projection) 是新一代的降维方法。\n",
    "\n",
    "**特点**：\n",
    "- 比 t-SNE 更快\n",
    "- 可以用于新数据转换\n",
    "- 更好地保持全局结构\n",
    "- 支持监督/半监督降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 安装: pip install umap-learn\ntry:\n    import umap\n    UMAP_AVAILABLE = True\n    print(f\"UMAP 已安装\")\nexcept ImportError:\n    UMAP_AVAILABLE = False\n    print(\"请先安装 umap: pip install umap-learn\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if UMAP_AVAILABLE:\n    import umap\n\n    # UMAP 降维\n    umap_reducer = umap.UMAP(n_components=2, random_state=42)\n    X_umap = umap_reducer.fit_transform(X_scaled)\n\n    print(f\"UMAP 输出形状: {X_umap.shape}\")\nelse:\n    print(\"跳过 UMAP（未安装）\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if UMAP_AVAILABLE:\n    # 三种方法对比\n    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n    # PCA\n    axes[0].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=y, cmap='tab10', alpha=0.7, s=10)\n    axes[0].set_title('PCA')\n\n    # t-SNE\n    axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', alpha=0.7, s=10)\n    axes[1].set_title('t-SNE')\n\n    # UMAP\n    scatter = axes[2].scatter(X_umap[:, 0], X_umap[:, 1], c=y, cmap='tab10', alpha=0.7, s=10)\n    axes[2].set_title('UMAP')\n\n    plt.colorbar(scatter, ax=axes[2], label='Digit')\n    plt.suptitle('Comparison of Dimensionality Reduction Methods')\n    plt.tight_layout()\n    plt.show()\nelse:\n    # 只对比 PCA 和 t-SNE\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n    axes[0].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=y, cmap='tab10', alpha=0.7, s=10)\n    axes[0].set_title('PCA')\n\n    scatter = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', alpha=0.7, s=10)\n    axes[1].set_title('t-SNE')\n\n    plt.colorbar(scatter, ax=axes[1], label='Digit')\n    plt.suptitle('Comparison of Dimensionality Reduction Methods (UMAP not available)')\n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 核 PCA (Kernel PCA)\n",
    "\n",
    "用于非线性降维的 PCA 变体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载鸢尾花数据\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "# 标准化\n",
    "X_iris_scaled = StandardScaler().fit_transform(X_iris)\n",
    "\n",
    "# 比较不同核的 Kernel PCA\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "\n",
    "for idx, kernel in enumerate(kernels):\n",
    "    kpca = KernelPCA(n_components=2, kernel=kernel, gamma=0.1)\n",
    "    X_kpca = kpca.fit_transform(X_iris_scaled)\n",
    "    \n",
    "    axes[idx].scatter(X_kpca[:, 0], X_kpca[:, 1], c=y_iris, cmap='viridis', alpha=0.8)\n",
    "    axes[idx].set_title(f'Kernel: {kernel}')\n",
    "    axes[idx].set_xlabel('Component 1')\n",
    "    axes[idx].set_ylabel('Component 2')\n",
    "\n",
    "plt.suptitle('Kernel PCA with Different Kernels')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 特征选择 vs 降维\n",
    "\n",
    "- **降维**：创建新特征（原特征的组合）\n",
    "- **特征选择**：选择原有特征的子集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 使用手写数字数据\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 方法1: SelectKBest (基于统计测试)\n",
    "selector_f = SelectKBest(f_classif, k=20)\n",
    "X_train_f = selector_f.fit_transform(X_train, y_train)\n",
    "X_test_f = selector_f.transform(X_test)\n",
    "\n",
    "# 方法2: 基于互信息\n",
    "selector_mi = SelectKBest(mutual_info_classif, k=20)\n",
    "X_train_mi = selector_mi.fit_transform(X_train, y_train)\n",
    "X_test_mi = selector_mi.transform(X_test)\n",
    "\n",
    "# 方法3: RFE (递归特征消除)\n",
    "rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "rfe = RFE(rf, n_features_to_select=20)\n",
    "X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
    "X_test_rfe = rfe.transform(X_test)\n",
    "\n",
    "print(f\"原始特征数: {X_train.shape[1]}\")\n",
    "print(f\"选择后特征数: {X_train_f.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比较不同方法的分类效果\n",
    "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# 原始数据\n",
    "clf.fit(X_train, y_train)\n",
    "acc_original = accuracy_score(y_test, clf.predict(X_test))\n",
    "\n",
    "# F-score 选择\n",
    "clf.fit(X_train_f, y_train)\n",
    "acc_f = accuracy_score(y_test, clf.predict(X_test_f))\n",
    "\n",
    "# 互信息选择\n",
    "clf.fit(X_train_mi, y_train)\n",
    "acc_mi = accuracy_score(y_test, clf.predict(X_test_mi))\n",
    "\n",
    "# RFE 选择\n",
    "clf.fit(X_train_rfe, y_train)\n",
    "acc_rfe = accuracy_score(y_test, clf.predict(X_test_rfe))\n",
    "\n",
    "# PCA 降维\n",
    "pca = PCA(n_components=20)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "clf.fit(X_train_pca, y_train)\n",
    "acc_pca = accuracy_score(y_test, clf.predict(X_test_pca))\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Method': ['Original (64)', 'F-score (20)', 'Mutual Info (20)', 'RFE (20)', 'PCA (20)'],\n",
    "    'Accuracy': [acc_original, acc_f, acc_mi, acc_rfe, acc_pca]\n",
    "})\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. 人脸数据降维实战\n\n> **注意**：以下代码使用 `fetch_olivetti_faces()` 加载 Olivetti 人脸数据集，首次运行时会自动从网络下载（约 3MB）。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载人脸数据\n",
    "faces = fetch_olivetti_faces()\n",
    "X_faces = faces.data\n",
    "y_faces = faces.target\n",
    "\n",
    "print(f\"数据形状: {X_faces.shape}\")\n",
    "print(f\"每张人脸是 64x64 = 4096 维\")\n",
    "print(f\"人数: {len(np.unique(y_faces))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显示部分人脸\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_faces[i*10].reshape(64, 64), cmap='gray')\n",
    "    ax.set_title(f'Person {y_faces[i*10]}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Sample Faces')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA 降维并可视化主成分（特征脸）\n",
    "pca_faces = PCA(n_components=50)\n",
    "X_faces_pca = pca_faces.fit_transform(X_faces)\n",
    "\n",
    "# 显示前 10 个特征脸\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(pca_faces.components_[i].reshape(64, 64), cmap='gray')\n",
    "    ax.set_title(f'PC {i+1}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Top 10 Eigenfaces (Principal Components)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 人脸重建\n",
    "n_components_list = [5, 10, 25, 50, 100, 200]\n",
    "\n",
    "fig, axes = plt.subplots(2, len(n_components_list)+1, figsize=(16, 5))\n",
    "\n",
    "# 选择两张人脸\n",
    "face_indices = [0, 100]\n",
    "\n",
    "for row, face_idx in enumerate(face_indices):\n",
    "    # 原图\n",
    "    axes[row, 0].imshow(X_faces[face_idx].reshape(64, 64), cmap='gray')\n",
    "    axes[row, 0].set_title('Original')\n",
    "    axes[row, 0].axis('off')\n",
    "    \n",
    "    # 不同成分数的重建\n",
    "    for col, n_comp in enumerate(n_components_list):\n",
    "        pca_temp = PCA(n_components=n_comp)\n",
    "        X_temp = pca_temp.fit_transform(X_faces)\n",
    "        X_reconstructed = pca_temp.inverse_transform(X_temp)\n",
    "        \n",
    "        axes[row, col+1].imshow(X_reconstructed[face_idx].reshape(64, 64), cmap='gray')\n",
    "        axes[row, col+1].set_title(f'n={n_comp}')\n",
    "        axes[row, col+1].axis('off')\n",
    "\n",
    "plt.suptitle('Face Reconstruction with Different Number of Components')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 练习题\n",
    "\n",
    "### 练习1：Fashion-MNIST 降维\n",
    "使用 t-SNE 对 Fashion-MNIST 数据集进行可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提示: 可以使用 sklearn.datasets 或 keras.datasets 加载数据\n",
    "# 在这里编写代码\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习2：PCA + 分类\n",
    "找出保持 95% 分类准确率所需的最少主成分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在这里编写代码\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 本课小结\n",
    "\n",
    "### 降维方法对比\n",
    "\n",
    "| 方法 | 类型 | 适用场景 | 特点 |\n",
    "|------|------|----------|------|\n",
    "| PCA | 线性 | 通用降维 | 快速，可解释 |\n",
    "| Kernel PCA | 非线性 | 非线性数据 | 处理非线性关系 |\n",
    "| t-SNE | 非线性 | 可视化 | 保持局部结构 |\n",
    "| UMAP | 非线性 | 可视化/降维 | 快速，保持全局结构 |\n",
    "\n",
    "### 使用建议\n",
    "\n",
    "1. **数据可视化**：优先使用 UMAP 或 t-SNE\n",
    "2. **特征压缩**：使用 PCA\n",
    "3. **分类任务预处理**：PCA 保留 95% 方差\n",
    "4. **非线性数据**：考虑 Kernel PCA 或 UMAP\n",
    "\n",
    "### 重要参数\n",
    "\n",
    "- **PCA**: n_components（成分数或方差比例）\n",
    "- **t-SNE**: perplexity（邻域大小，5-50）\n",
    "- **UMAP**: n_neighbors、min_dist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}